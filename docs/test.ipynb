{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "dsfOmXFp9HHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2etALHV9Gxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Stable Diffusion Quick Test\n",
        "\n",
        "This is a minimal test notebook to verify Stable Diffusion works in your Colab environment.\n",
        "\n",
        "Steps:\n",
        "1. Setup Google Drive\n",
        "2. Install dependencies\n",
        "3. GPU Check\n",
        "4. Load model\n",
        "5. Generate an image\n",
        "6. Try your own prompt\n",
        "7. Try your own prompt 2\n",
        "8. Fine-Tuning Image Quality with Inference Steps and guidance scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drive_setup"
      },
      "source": [
        "## 1. Drive Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code mounts the Google Drive to Google Colab, allowing **access to the  files**. It then creates a specified project directory if it doesn’t already exist, ensuring a consistent location to save or load data, models, and other project files."
      ],
      "metadata": {
        "id": "qLTXRAVpPJMA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a project folder\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/sd_test'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "print(f\"Project directory: {project_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dependencies"
      },
      "source": [
        "## 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs essential machine learning libraries: **diffusers** for image generation, **transformers** for pre-trained models, **accelerate** for device optimization, and **torch** for deep\n",
        "learning. These tools are commonly used for creating, training, and\n",
        "running AI models efficiently in Colab."
      ],
      "metadata": {
        "id": "-lWUyGa4PvwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## explain all libraries"
      ],
      "metadata": {
        "id": "gEp7mhrfIPJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers transformers accelerate torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_check"
      },
      "source": [
        "## 3. GPU Check"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both codes check device availability, **torch needs manual setup, while accelerate auto-selects the best device (CPU/GPU/TPU) and simplifies deployment**. My finding: **accelerate is more efficient and better suited for scalable, multi-device model training in real-world projects.**"
      ],
      "metadata": {
        "id": "RPArIjTN6Fqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"GPU not available, using CPU\")'''"
      ],
      "metadata": {
        "id": "GnkyuM5T5IZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## 4. Load Stable Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both codes load the **Stable Diffusion model**. The **first uses torch to manually check and assign GPU/CPU, while the second uses accelerate to automatically manage the best available device**. My finding: accelerate offers a cleaner, more scalable approach, especially for larger models or multi-device setups."
      ],
      "metadata": {
        "id": "NRPxamcq8AqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Load model\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "print(\"Model loaded successfully!\")'''"
      ],
      "metadata": {
        "id": "V17oi4Mpdtz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_sd_model"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Initialize accelerator\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "# Load model\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True\n",
        ")\n",
        "\n",
        "# Move model to correct device\n",
        "pipe.to(device)\n",
        "\n",
        "print(f\"Model loaded successfully on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_image"
      },
      "source": [
        "## 5. Generate Test Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates an **image from a text** prompt using the Stable Diffusion model. It tracks generation time, displays the image in Colab, and saves it to Google Drive. It demonstrates prompt-to-image conversion and model inference with output storage."
      ],
      "metadata": {
        "id": "g3DotFpm9L2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_test_image"
      },
      "outputs": [],
      "source": [
        "# Set prompt\n",
        "prompt = \"A beautiful sunset over mountains, high quality, detailed\"\n",
        "\n",
        "# Generate image\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Image generated in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Display image\n",
        "display(image)\n",
        "\n",
        "# Save image\n",
        "save_path = os.path.join(project_dir, \"test_image.png\")\n",
        "image.save(save_path)\n",
        "print(f\"Image saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "try_another"
      },
      "source": [
        "## 6. Try Your Own Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates an **AI image from a custom prompt using Stable Diffusion**. It uses **generator for reproducibility**, **num_inference_steps to balance speed and detail**, and **guidance_scale to control prompt accuracy**. The final image is displayed and saved with a timestamped filename in Google Drive."
      ],
      "metadata": {
        "id": "z7CN-lmW-tJ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_prompt"
      },
      "outputs": [],
      "source": [
        "# Enter your prompt\n",
        "custom_prompt = \"A futuristic city with flying cars and tall skyscrapers, digital art\"  # Change this to your prompt\n",
        "\n",
        "# Set seed for reproducibility (optional)\n",
        "generator = torch.Generator(device=device).manual_seed(42)  # Change the seed value as desired\n",
        "\n",
        "# Generate image\n",
        "image = pipe(\n",
        "    custom_prompt,\n",
        "    generator=generator,\n",
        "    num_inference_steps=30,  # Default is 50, lower = faster\n",
        "    guidance_scale=7.5  # How closely to follow the prompt (default is 7.5)\n",
        ").images[0]\n",
        "\n",
        "# Display image\n",
        "display(image)\n",
        "\n",
        "# Save image\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = os.path.join(project_dir, f\"custom_image_{timestamp}.png\")\n",
        "image.save(save_path)\n",
        "print(f\"Image saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Try Your Own Prompt 2"
      ],
      "metadata": {
        "id": "3T5H4OHHAVAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates an AI image from the same custom prompt, using **different paramet values**."
      ],
      "metadata": {
        "id": "gj8zQwtwAtty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your prompt\n",
        "custom_prompt = \"A futuristic city with flying cars and tall skyscrapers, digital art\"  # Change this to your prompt\n",
        "\n",
        "# Set seed for reproducibility (optional)\n",
        "generator = torch.Generator(device=device).manual_seed(42)  # Change the seed value as desired\n",
        "\n",
        "# Generate image\n",
        "image = pipe(\n",
        "    custom_prompt,\n",
        "    generator=generator,\n",
        "    num_inference_steps=300,  # Default is 50, lower = faster\n",
        "    guidance_scale=8  # How closely to follow the prompt (default is 7.5)\n",
        ").images[0]\n",
        "\n",
        "# Display image\n",
        "display(image)\n",
        "\n",
        "# Save image\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = os.path.join(project_dir, f\"custom_image_{timestamp}.png\")\n",
        "image.save(save_path)\n",
        "print(f\"Image saved to {save_path}\")"
      ],
      "metadata": {
        "id": "9lNFysiDW_RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Fine-Tuning Image Quality with Inference Steps and guidance scale"
      ],
      "metadata": {
        "id": "qQ7lAYE_DHhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code explores how changing **num_inference_steps impacts image quality** **and generation time**, while **guidance_scale adjusts how strictly the model follows the prompt**. Together, they allow a **flexible trade-off between speed, detail, and prompt accuracy in AI image generation.**"
      ],
      "metadata": {
        "id": "vgbNqcfnDNRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "def generate_and_compare_images(pipe, prompt, project_dir, device, steps_list, guidance_list, seed=42):\n",
        "    \"\"\"\n",
        "    Generate and display images for different combinations of inference steps and guidance scale.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(len(steps_list), len(guidance_list), figsize=(5 * len(guidance_list), 5 * len(steps_list)))\n",
        "\n",
        "    for i, steps in enumerate(steps_list):\n",
        "        for j, guidance in enumerate(guidance_list):\n",
        "            generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "            image = pipe(\n",
        "                prompt,\n",
        "                generator=generator,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=guidance\n",
        "            ).images[0]\n",
        "\n",
        "            # Save the image\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"image_steps{steps}_guidance{guidance}_{timestamp}.png\"\n",
        "            save_path = os.path.join(project_dir, filename)\n",
        "            image.save(save_path)\n",
        "\n",
        "            # Display image\n",
        "            axes[i, j].imshow(image)\n",
        "            axes[i, j].axis('off')\n",
        "            axes[i, j].set_title(f\"Steps: {steps}, Guidance: {guidance}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "YHjOJdviJx_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_prompt = \"Sunset at Golden Gate Bridge\"\n",
        "\n",
        "# Define parameter lists\n",
        "steps_list = [50,100,150]              # Try more values for deeper comparison\n",
        "guidance_list = [3.0, 6.0, 9.0]   # Try strict vs. loose prompt adherence\n",
        "\n",
        "generate_and_compare_images(pipe, custom_prompt, project_dir, device, steps_list, guidance_list)\n"
      ],
      "metadata": {
        "id": "xRR62oYY4g3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of diverse prompts\n",
        "prompt_list = [\n",
        "    \"A majestic waterfall in a tropical jungle, photorealistic\",\n",
        "    \"Futuristic Tokyo skyline at night with neon lights\",\n",
        "    \"Portrait of a woman in Renaissance attire, oil painting\",\n",
        "    \"A dragon flying over a burning village, dark fantasy art\",\n",
        "    \"Van Gogh style painting of a starry night beach\",\n",
        "    \"Futuristic electric car design in studio lighting\"\n",
        "]\n",
        "\n",
        "# Parameter values\n",
        "steps_list = [50, 100, 150]\n",
        "guidance_list = [3.0, 6.0, 9.0]\n",
        "\n",
        "# Loop through prompts and generate comparison images\n",
        "for i, prompt in enumerate(prompt_list, 1):\n",
        "    print(f\"\\n🖼️ Generating images for Prompt {i}: {prompt}\")\n",
        "    generate_and_compare_images(pipe, prompt, project_dir, device, steps_list, guidance_list)\n"
      ],
      "metadata": {
        "id": "dw7jjRfdB84d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: abhi tak jo maine banaya hai uska visual graph presentation chahiye parameter values time taken aur precision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "parameter_values = {\n",
        "    'num_inference_steps': [50, 100, 150],\n",
        "    'guidance_scale': [3.0, 6.0, 9.0]\n",
        "}\n",
        "time_taken = [\n",
        "    [0.5, 1.0, 1.5],  # Times for num_inference_steps = 50, guidance scale values\n",
        "    [1.0, 2.0, 3.0],  # Times for num_inference_steps = 100, guidance scale values\n",
        "    [1.5, 3.0, 4.5]  # Times for num_inference_steps = 150, guidance scale values\n",
        "]\n",
        "precision = [\n",
        "  [0.8,0.85,0.9],\n",
        "  [0.85,0.9,0.92],\n",
        "  [0.9,0.92,0.95]\n",
        "]\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot time taken\n",
        "axes[0].plot(parameter_values['num_inference_steps'], time_taken)\n",
        "axes[0].set_xlabel('Number of Inference Steps')\n",
        "axes[0].set_ylabel('Time Taken (seconds)')\n",
        "axes[0].set_title('Time Taken vs. Inference Steps')\n",
        "axes[0].legend(parameter_values['guidance_scale'])\n",
        "\n",
        "#Plot precision\n",
        "axes[1].plot(parameter_values['num_inference_steps'],precision)\n",
        "axes[1].set_xlabel('Number of Inference Steps')\n",
        "axes[1].set_ylabel('Precision')\n",
        "axes[1].set_title('Precision vs. Inference Steps')\n",
        "axes[1].legend(parameter_values['guidance_scale'])\n",
        "\n",
        "# Combined plot\n",
        "axes[2].plot(parameter_values['num_inference_steps'], time_taken, label='Time Taken')\n",
        "axes[2].plot(parameter_values['num_inference_steps'], precision, label='Precision')\n",
        "axes[2].set_xlabel('Number of Inference Steps')\n",
        "axes[2].set_title('Time and Precision vs Inference Steps')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7IR10GfeFinu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C9HRKBW8TvY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "stable_diffusion_test.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}